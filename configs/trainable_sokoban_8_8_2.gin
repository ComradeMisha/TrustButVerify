# Config for stable Sokoban (8, 8, 2) with TrainableEnv and
# DeterministicMCTSAgent. Experiment takes around 6 hours to complete on the
# Eagle.
#
# Parameters for Runner:
# ==============================================================================
Runner.agent_class = @alpacka.agents.DeterministicMCTSAgent
Runner.batch_stepper_class = @alpacka.batch_steppers.LocalBatchStepper
Runner.env_class = @alpacka.envs.Sokoban
Runner.episode_time_limit = 50
Runner.n_envs = 64
Runner.n_epochs = 1500
Runner.n_model_precollect_episodes = 5e4
Runner.n_model_pretrain_epochs = 200
Runner.n_precollect_epochs = 20
Runner.fine_tune_on_agent_data = True
Runner.log_rollout_every_n_epochs = 50
Runner.network_class = @agent/alpacka.networks.KerasNetwork
Runner.trainer_class = @agent/alpacka.trainers.SupervisedTrainer
Runner.model_class = @alpacka.envs.TrainableSokoban
Runner.model_network_class = @model/alpacka.networks.KerasNetwork
Runner.model_trainer_class = @model/alpacka.trainers.SupervisedTrainer

# Parameters for Model SupervisedTrainer:
# ==============================================================================
model/SupervisedTrainer.batch_size = 64
model/SupervisedTrainer.replay_buffer_capacity = 2e6
model/SupervisedTrainer.validation_replay_buffer_capacity = 2e4
model/SupervisedTrainer.n_steps_per_epoch = 50
model/SupervisedTrainer.validation_split = 0.01
model/SupervisedTrainer.validate_every_n_epochs = 50
model/SupervisedTrainer.replay_buffer_sampling_hierarchy = ['solved']
model/SupervisedTrainer.inputs = @alpacka.trainers.supervised.input_observation_and_action
model/SupervisedTrainer.target = @alpacka.trainers.supervised.target_model

# Parameters for fcn_with_reward_function:
# ==============================================================================
fcn_with_reward_function.cnn_channels = 64
fcn_with_reward_function.cnn_kernel_size = (5, 5)
fcn_with_reward_function.cnn_n_layers = 2
fcn_with_reward_function.batch_norm = True
fcn_with_reward_function.global_average_pooling = True
fcn_with_reward_function.output_activation = {
    'next_observation': 'softmax',
    'reward': 'sigmoid',
    'done': 'sigmoid'
}

# Parameters for scheduling learning rate for model:
# ==============================================================================
model/two_step_lr.init_lr = 1e-3
model/two_step_lr.target_lr = 5e-5
model/two_step_lr.n_init_epochs = 40
model/LearningRateScheduler.schedule = @model/alpacka.networks.keras.two_step_lr

# Parameters for Model KerasNetwork:
# ==============================================================================
model/KerasNetwork.weight_decay = 1e-5
model/KerasNetwork.loss = {
    'next_observation': @categorical_crossentropy,
    'reward': @binary_crossentropy,
    'done': @binary_crossentropy
}
model/KerasNetwork.model_fn = @alpacka.networks.keras.fcn_with_reward_function
model/KerasNetwork.optimizer = 'adam'
model/KerasNetwork.train_callbacks = [
    @model/tf.keras.callbacks.LearningRateScheduler(),
]
model/KerasNetwork.metrics = {
    'next_observation': ['accuracy',@alpacka.networks.keras.PerfectNextObservation()],
    'reward': ['accuracy', @reward/Recall(), @reward/Precision()],
    'done': ['accuracy', @done/Recall(), @done/Precision()],
}

# Parameters for convnet_mnist:
# ==============================================================================
convnet_mnist.activation = 'relu'
convnet_mnist.d_conv = 64
convnet_mnist.d_ff = 128
convnet_mnist.n_conv_layers = 5
convnet_mnist.output_activation = None

# Parameters for Agent KerasNetwork:
# ==============================================================================
agent/KerasNetwork.loss = 'mean_squared_error'
agent/KerasNetwork.loss_weights = None
agent/KerasNetwork.metrics = ['mae']
agent/KerasNetwork.model_fn = @alpacka.networks.keras.convnet_mnist
agent/KerasNetwork.optimizer = 'adam'
agent/KerasNetwork.train_callbacks = None
agent/KerasNetwork.weight_decay = 0.003

# Parameters for Agent SupervisedTrainer:
# ==============================================================================
agent/SupervisedTrainer.batch_size = 32
agent/SupervisedTrainer.n_steps_per_epoch = 13
agent/SupervisedTrainer.replay_buffer_capacity = 5e5
agent/SupervisedTrainer.replay_buffer_sampling_hierarchy = ['solved']
agent/SupervisedTrainer.target = @alpacka.trainers.supervised.target_value

# Parameters for DeterministicMCTSAgent:
# ==============================================================================
DeterministicMCTSAgent.avoid_loops = True
DeterministicMCTSAgent.gamma = 0.99
DeterministicMCTSAgent.n_passes = 10
DeterministicMCTSAgent.value_traits_class = @alpacka.agents.deterministic_mcts.ScalarValueTraits
DeterministicMCTSAgent.value_accumulator_class = @alpacka.agents.deterministic_mcts.ScalarValueAccumulator
DeterministicMCTSAgent.render_rollout = True

# Parameters for ScalarValueTraits:
# ==============================================================================
ScalarValueTraits.dead_end_value = -0.2

# Parameters for Sokoban:
# ==============================================================================
Sokoban.dim_room = (8, 8)
Sokoban.num_boxes = 2
Sokoban.penalty_for_step = 0
Sokoban.reward_box_on_target = 0
Sokoban.reward_finished = 1

# Parameters for input_observation_and_action:
# ==============================================================================
input_observation_and_action.actions_space_size = 4

# Parameters for Model metrics:
# ==============================================================================
done/Precision.name = 'precision_done'
done/Recall.name = 'recall_done'
next_observation/Precision.name = 'precision_next_observation'
next_observation/Recall.name = 'recall_next_observation'
reward/Precision.name = 'precision_reward'
reward/Recall.name = 'recall_reward'
