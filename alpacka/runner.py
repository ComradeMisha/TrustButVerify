"""Entrypoint of the experiment."""

import argparse
import functools
import itertools
import os
import time

import gin
import numpy as np

from alpacka import agents
from alpacka import batch_steppers
from alpacka import envs
from alpacka import metric_logging
from alpacka import networks
from alpacka import trainers
from alpacka.agents import RandomAgent
from alpacka.data import nested_map
from alpacka.data import nested_unstack
from alpacka.envs import get_model_network_signature
from alpacka.utils import gin as gin_utils
from alpacka.utils.experienced_states_logging import ExperiencedStatesLogger
from alpacka.utils.graph_distance_logging import GraphDistanceLogger
from alpacka.utils.mrunner_client import NeptuneAPITokenException


@gin.configurable
class Runner:
    """Main class running the experiment."""

    def __init__(
        self,
        output_dir,
        env_class=envs.CartPole,
        env_kwargs=None,
        agent_class=agents.RandomAgent,
        network_class=networks.DummyNetwork,
        model_class=None,
        model_network_class=None,
        n_envs=16,
        n_envs_random_episodes=None,
        episode_time_limit=None,
        batch_stepper_class=batch_steppers.LocalBatchStepper,
        trainer_class=trainers.DummyTrainer,
        model_trainer_class=None,
        n_epochs=None,
        n_model_precollect_episodes=0,
        n_model_pretrain_epochs=0,
        n_precollect_epochs=0,
        fine_tune_on_agent_data=False,
        log_rollout_every_n_epochs=None,
        log_heat_map_every_n_epochs=None,
        log_other_images_every_n_epochs=None,
        log_detailed_heat_map=False,
        log_false_positive_images=False,
        rolling_metrics=None,
        clear_model_replay_after_pretrain=False,
        agent_reset_schedule=None,
        log_n_top_transitions=None,
        log_graph_distances=True,
        log_n_experienced_states=True,
    ):
        """Initializes the runner.

        Args:
            output_dir (str): Output directory for the experiment.
            env_class (type): Environment class.
            env_kwargs (dict): Keyword arguments to pass to the env class
                when created. It ensures that only the env in the Runner will be
                initialized with them.
            agent_class (type): Agent class.
            network_class (type): Agent network class.
            model_class (type): Model class.
            model_network_class (type): Model network class.
            n_envs (int): Number of environments to run in parallel.
            n_envs_random_episodes (int): Number of environments to generate
                random episodes in parallel. If not specified, n_envs is used.
            episode_time_limit (int or None): Time limit for solving an episode.
                None means no time limit.
            batch_stepper_class (type): BatchStepper class.
            trainer_class (type): Agent trainer class.
            model_trainer_class (type): Model trainer class.
            trainer_class (type): Trainer class.
            n_epochs (int or None): Number of epochs to run for, or indefinitely
                if None.
            n_model_precollect_episodes (int): Number of episodes to collect
                from RandomAgent run before starting model training.
            n_model_pretrain_epochs (int): Number of epochs to run model
                training (without trained agent rollouts).
            n_precollect_epochs (int): Number of epochs to run without
                training agent, but with trained model.
            fine_tune_on_agent_data (bool): Specifies if model should be fine
                tuned on episodes generated by the agent.
            log_rollout_every_n_epochs (Optional[int]): Frequency of agent
                rollout logging.
            log_heat_map_every_n_epochs (Optional[int]): Frequency of agent
                heat map logging.
            log_other_images_every_n_epochs (Optional[int]): Frequency of agent
                other images logging.
            log_detailed_heat_map (bool): Specifies if detailed heat maps should
                be logged. Detailed heat map has breakdown by number of keys
                taken.
            log_false_positive_images(bool): Specifies if call
                log_transitions_with_false_positive_rewards method
            rolling_metrics (Optional[List[ExperimentMetrics]]): List of
                metrics that should be computed across the epochs.
            clear_model_replay_after_pretrain (bool): Indicates if model replay
                buffer should be cleared after pretrain phase.
            agent_reset_schedule (Optional[List[int]]): Schedule for resetting
                agent weights and replay buffer. List of epochs when reset
                will be performed
            log_n_top_transitions (Optional[int]): Number of top transitions
                in priority replay buffer that should be logged.
            log_graph_distances (bool): If graph distance statistics should be
                logged. Use it for only deterministic envs, with only reward
                related to reaching the goal.
            log_n_experienced_states (bool): If number of unique experienced
                states and transitions should be logged.

        Raises:
            ValueError: On partial specification of model's network related
                classes.

        """
        self._output_dir = os.path.expanduser(output_dir)
        os.makedirs(self._output_dir, exist_ok=True)

        network_signature = self._infer_network_signature(
            env_class, agent_class
        )
        network_fn = functools.partial(
            network_class, network_signature=network_signature
        )

        if env_kwargs:
            env_fn = functools.partial(env_class, **env_kwargs)
        else:
            env_fn = env_class

        self._agent_class = agent_class
        self._network = network_fn()
        self._trainer = trainer_class(network_signature)

        model_network_fn = None
        self._model_network = None
        self._model_trainer = None
        self._random_episodes_batch_stepper = None
        if model_network_class is not None:
            if model_trainer_class is None:
                raise ValueError(
                    'Model trainer class should be specified in '
                    'an experiment with a trainable model.'
                )
            model_network_signature = self._infer_model_network_signature(
                env_class
            )

            model_network_fn = functools.partial(
                model_network_class, network_signature=model_network_signature
            )

            self._model_network = model_network_fn()
            self._model_trainer = model_trainer_class(model_network_signature)
            self._random_episodes_batch_stepper = batch_stepper_class(
                env_class=env_fn,
                agent_class=RandomAgent,
                network_fn=None,
                n_envs=n_envs_random_episodes or n_envs,
                output_dir=self._output_dir,
            )

        self._batch_stepper = batch_stepper_class(
            env_class=env_fn,
            agent_class=agent_class,
            network_fn=network_fn,
            n_envs=n_envs,
            output_dir=self._output_dir,
            model_class=model_class,
            model_network_fn=model_network_fn
        )
        self._episode_time_limit = episode_time_limit

        self._n_epochs = n_epochs
        self._n_model_precollect_epochs = int(
            n_model_precollect_episodes // n_envs
        )
        self._n_model_pretrain_epochs = n_model_pretrain_epochs
        self._n_precollect_epochs = n_precollect_epochs
        if fine_tune_on_agent_data and model_network_class is None:
            # If model_network_class is set, then all other parameters have
            # already been checked.
            raise ValueError(
                'Fine tuning TrainableModel requires that all TrainableModel '
                'parameters are set, but model_network_class is not set.'
            )
        self._fine_tune_on_agent_data = fine_tune_on_agent_data
        self._log_rollout_every_n_epochs = log_rollout_every_n_epochs
        self._log_heat_map_every_n_epochs = log_heat_map_every_n_epochs
        self._log_other_images_every_n_epochs = log_other_images_every_n_epochs
        self._log_detailed_heat_map = log_detailed_heat_map
        self._log_false_positive_images = log_false_positive_images

        self._rolling_metrics = rolling_metrics or []

        self._epoch = 0
        self._total_episodes = 0
        self._agent_reset_schedule = agent_reset_schedule
        self._clear_model_replay_after_pretrain = \
            clear_model_replay_after_pretrain
        self._last_agent_reset = 0

        if model_class is not None:
            self._logging_env = model_class(modeled_env=env_fn())
        else:
            self._logging_env = env_fn()
        self._log_n_top_transitions = log_n_top_transitions
        self._log_graph_distances = log_graph_distances
        if self._log_graph_distances:
            self._graph_logger = GraphDistanceLogger(env_fn())
        self._log_n_experienced_states = log_n_experienced_states
        if self._log_n_experienced_states:
            self._experienced_states_logger = ExperiencedStatesLogger(
                obs2state_fn=env_fn().obs2state)

    @staticmethod
    def _infer_network_signature(env_class, agent_class):
        # Initialize an environment and an agent to get a network signature.
        env = env_class()
        agent = agent_class()
        return agent.network_signature(env.observation_space, env.action_space)

    @staticmethod
    def _infer_model_network_signature(env_class):
        env = env_class()
        return get_model_network_signature(
            env.observation_space, env.action_space
        )

    def _compute_episode_metrics(self, episodes):
        metrics = {}

        return_mean = sum(
            episode.return_ for episode in episodes
        ) / len(episodes)
        metrics['return_mean'] = return_mean

        metrics['length_mean'] = sum(
            episode.transition_batch.reward.shape[0] for episode in episodes
        ) / len(episodes)

        metrics['solved_rate'] = sum(
            int(episode.solved) for episode in episodes
            if episode.solved is not None
        ) / len(episodes)
        metrics['count'] = self._total_episodes
        return metrics

    def _compute_loop_metrics_one_ep(self,
                                     predicted_next_obs,
                                     current_obs,
                                     real_next_obs):

        predicted_loop_mask = np.array(
            (predicted_next_obs == current_obs).all(axis=1).astype(int)
        )
        real_loop_mask = np.array(
            (current_obs == real_next_obs).all(axis=1).astype(int)
        )
        predicted_loop_true_mask = predicted_loop_mask & real_loop_mask
        predicted_loop_false_mask = predicted_loop_mask & ~real_loop_mask
        def to_freq(mask):
            return np.sum(mask) / len(mask) if len(mask) > 0 else 0
        loop_metrics_one_ep = {
            'predicted_loop_events' : np.sum(predicted_loop_mask),
            'predicted_loop_true_events' : np.sum(predicted_loop_true_mask),
            'predicted_loop_false_event' : np.sum(predicted_loop_false_mask),
            'predicted_loop_freq' : to_freq(predicted_loop_mask),
            'predicted_loop_true_freq' : to_freq(predicted_loop_true_mask),
            'predicted_loop_false_freq' : to_freq(predicted_loop_false_mask)
        }
        return loop_metrics_one_ep

    def _compute_hit_wall_metrics_ep(self,
                                     hit_wall_transitions_mask,
                                     random_actions_mask,
                                     novelty_based_actions_mask,
                                     bonus_based_actions_mask
                                  ):
        one_episode_metrics = {}
        all_hits = hit_wall_transitions_mask.sum()
        random_hit_wall_mask = hit_wall_transitions_mask & \
                               random_actions_mask
        all_random_hits = np.sum(random_hit_wall_mask)
        hits_by_mcts = np.sum(hit_wall_transitions_mask & \
                       ~random_actions_mask & \
                       ~novelty_based_actions_mask)

        one_episode_metrics['bonus_actions'] = \
            np.sum(bonus_based_actions_mask)
        one_episode_metrics['bonus_actions_freq'] = \
            np.mean(bonus_based_actions_mask)

        one_episode_metrics['hit_events'] = all_hits
        one_episode_metrics['hit_freq'] = hit_wall_transitions_mask.mean()
        one_episode_metrics['hits_random'] = all_random_hits
        one_episode_metrics['hits_random_freq'] = (
            all_random_hits / all_hits if all_hits > 0 else 0
        )
        one_episode_metrics['hits_by_mcts'] = hits_by_mcts
        one_episode_metrics['hits_by_mcts_freq'] = (
            hits_by_mcts / all_hits if all_hits > 0 else 0
        )

        hit_wall_novelty_based_mask = hit_wall_transitions_mask & \
                                       novelty_based_actions_mask
        one_episode_metrics['novelty_based_transitions'] = \
            np.sum(novelty_based_actions_mask)
        one_episode_metrics['novelty_based_transitions_freq'] = \
            np.mean(novelty_based_actions_mask)
        one_episode_metrics['novelty_based_hits'] = \
            np.sum(hit_wall_novelty_based_mask)
        one_episode_metrics['novelty_based_hits_freq'] = \
            np.sum(hit_wall_novelty_based_mask) / all_hits if \
                all_hits > 0 else 0

        return one_episode_metrics

    def _compute_metrics_across_epochs(self, episodes):
        for metric in self._rolling_metrics:
            metric.update_state(self._epoch, episodes)

        return {
            str(metric): metric.result(self._epoch)
            for metric in self._rolling_metrics
        }

    def _compute_trainable_model_metrics(self, episodes):

        def merge_metrics_over_episodes(total_stat, one_ep_stat):
            for metric_name, value in one_ep_stat.items():
                if metric_name in total_stat:
                    total_stat[metric_name].append(value)
                else:
                    total_stat[metric_name] = [value]

        def average_over_episodes(stats):
            return {
                metric_name: np.mean(metric_values)
                for metric_name, metric_values in stats.items()
            }

        predicted_next_observations = []
        predicted_rewards = []
        for episode in episodes:
            predicted_next_observations_i = np.empty_like(
                episode.transition_batch.next_observation
            )
            predicted_rewards_i = np.empty_like(
                episode.transition_batch.reward
            )
            for t, (action_t, observations_t, rewards_t) in enumerate(zip(
                    episode.transition_batch.action,
                    episode.transition_batch.agent_info[
                        'children_observations'],
                    episode.transition_batch.agent_info['children_rewards']
            )):
                predicted_next_observations_i[t] = observations_t[action_t]
                predicted_rewards_i[t] = rewards_t[action_t]
            predicted_next_observations.append(predicted_next_observations_i)
            predicted_rewards.append(predicted_rewards_i)

        predictions = []
        perfect_next_observations_log = []
        hit_wall_stats = {}
        loop_stats = {}
        for episode, predicted_next_observations_i in zip(
                episodes, predicted_next_observations
        ):
            prediction = np.equal(
                predicted_next_observations_i,
                episode.transition_batch.next_observation
            )
            predictions.append(prediction)
            perfect_next_observations_log.extend(
                np.all(prediction, axis=tuple(range(1, prediction.ndim))))
            if 'bonus_based_action' in episode.transition_batch.agent_info:
                # calculate hit wall metrics
                bonus_based_actions_mask = \
                    episode.transition_batch.agent_info['bonus_based_action']
                novelty_based_actions_mask = \
                    episode.transition_batch.agent_info['novelty_based_action']
                random_actions_mask = \
                    episode.transition_batch.agent_info['random_action']
                hit_wall_transitions_mask = np.array(
                    (episode.transition_batch.observation ==
                     episode.transition_batch.next_observation
                     ).all(axis=1).astype(int))

                hit_wall_metrics_one_ep = self._compute_hit_wall_metrics_ep(
                    hit_wall_transitions_mask, random_actions_mask,
                    novelty_based_actions_mask, bonus_based_actions_mask
                )
                merge_metrics_over_episodes(
                    hit_wall_stats, hit_wall_metrics_one_ep)

            loop_metrics_one_ep = self._compute_loop_metrics_one_ep(
                predicted_next_observations_i,
                episode.transition_batch.observation,
                episode.transition_batch.next_observation
            )
            merge_metrics_over_episodes(loop_stats, loop_metrics_one_ep)

        metrics = {}
        metrics.update(average_over_episodes(hit_wall_stats))
        metrics.update(average_over_episodes(loop_stats))
        metrics.update({
            metric_name: np.mean(metric_values)
            for metric_name, metric_values in hit_wall_stats.items()
        })
        metrics['perfect_next_observation'] = np.mean(
            perfect_next_observations_log
        )

        total_positives = sum(
            sum(episode.transition_batch.reward) for episode in episodes
        )
        total_predicted_positives = sum(
            sum(predicted_reward) for predicted_reward in predicted_rewards
        )
        total_true_positives = sum(
            sum(predicted_reward * episode.transition_batch.reward)
            for episode, predicted_reward in zip(episodes, predicted_rewards)
        )

        if total_true_positives == 0:
            metrics['predicted_reward_recall'] = 0.
            metrics['predicted_reward_precision'] = 0.
        else:
            metrics['predicted_reward_recall'] = (
                total_true_positives / total_positives
            )
            metrics['predicted_reward_precision'] = (
                total_true_positives / total_predicted_positives
            )

        return metrics

    @staticmethod
    def _select_false_positive_transitions(transitions, fp_mask):
        fp_transitions_indices = np.squeeze(np.argwhere(fp_mask == 1), axis=1)
        return np.take(transitions, fp_transitions_indices, axis=0)

    def _log_transitions_with_false_positive_rewards(self, episodes):
        predicted_rewards = []
        for episode in episodes:
            predicted_rewards_i = np.empty_like(
                episode.transition_batch.reward
            )
            for t, (action_t, rewards_t) in enumerate(zip(
                episode.transition_batch.action,
                episode.transition_batch.agent_info['children_rewards']
            )):
                predicted_rewards_i[t] = rewards_t[action_t]
            predicted_rewards.append(predicted_rewards_i)
        false_positives_mask = [
            (predicted_reward *
             (1 - episode.transition_batch.reward))
            for episode, predicted_reward in zip(episodes, predicted_rewards)
        ]

        fp_transitions = [
            nested_map(
                functools.partial(
                    self._select_false_positive_transitions, fp_mask=fp_mask
                ),
                episode.transition_batch
            )
            for episode, fp_mask in zip(episodes, false_positives_mask)
            if fp_mask.any()
        ]

        images_logged = 0
        for fp_transition in fp_transitions:
            transition_visualizations = self._logging_env.visualize_transitions(
                fp_transition
            )

            for transition_viz in transition_visualizations:
                metric_logging.log_image(
                    f'episode_model/fp_transition_{self._epoch}',
                    images_logged, transition_viz
                )
                images_logged += 1

                if images_logged > 50:
                    break
            if images_logged > 50:
                break

    def _save_gin(self):
        config_path = os.path.join(self._output_dir, 'config.gin')
        config_str = gin.operative_config_str()
        with open(config_path, 'w') as f:
            f.write(config_str)

        for (name, value) in gin_utils.extract_bindings(config_str):
            metric_logging.log_property(name, value)

    def _generate_random_episodes(self):
        log_every_n_epochs = max(1, self._n_model_precollect_epochs // 10)

        random_episodes_metrics_helper = {
            'return_total': 0,
            'length_total': 0,
            'solved': 0,
            'count': 0
        }

        for i in range(self._n_model_precollect_epochs):
            episodes = self._random_episodes_batch_stepper.run_episode_batch(
                agent_params=None, model_params=None, epoch=0,
                time_limit=self._episode_time_limit
            )
            # Calculate metrics and update metrics helper.
            episodes_batch_metrics = self._compute_episode_metrics(episodes)
            random_episodes_metrics_helper['count'] += len(episodes)
            random_episodes_metrics_helper['return_total'] += \
                episodes_batch_metrics['return_mean'] * len(episodes)
            random_episodes_metrics_helper['length_total'] += \
                episodes_batch_metrics['length_mean'] * len(episodes)
            random_episodes_metrics_helper['solved'] += \
                episodes_batch_metrics['solved_rate'] * len(episodes)

            # Add episodes to the replay buffer.
            for episode in episodes:
                self._model_trainer.add_episode(
                    episode, {'random_episode': True}
                )

            if i % log_every_n_epochs == log_every_n_epochs - 1:
                metric_logging.log_scalar(
                    'random_episodes/generated',
                    step=i,
                    value=i / self._n_model_precollect_epochs
                )

        metric_logging.log_scalar(
            'random_episodes/generated',
            step=self._n_model_precollect_epochs,
            value=1.0
        )

        random_episodes_metrics = {
            'return_mean':
                random_episodes_metrics_helper['return_total'] /
                random_episodes_metrics_helper['count'],
            'length_mean':
                random_episodes_metrics_helper['length_total'] /
                random_episodes_metrics_helper['count'],
            'solved_rate':
                random_episodes_metrics_helper['solved'] /
                random_episodes_metrics_helper['count'],
            'count': random_episodes_metrics_helper['count'],
        }
        metric_logging.log_scalar_metrics(
            'random_episodes',
            self._epoch,
            random_episodes_metrics
        )

    def run_epoch(self):
        """Runs a single epoch."""
        metric_logging.log_scalar(
            'epoch',
            self._epoch,
            self._epoch,
        )
        episodes = self._run_episode_batch()
        self._total_episodes += len(episodes)
        self._log_episodes_metrics_and_images(episodes)

        for episode in episodes:
            self._trainer.add_episode(episode)

        if self._fine_tune_on_agent_data:
            for episode in episodes:
                self._model_trainer.add_episode(
                    episode, {'random_episode': False}
                )

        if (self._agent_reset_schedule is not None and
                self._epoch in self._agent_reset_schedule):
            self._network.reset()
            self._trainer.clear_experience()
            self._last_agent_reset = self._epoch

        if self._epoch >= self._last_agent_reset + self._n_precollect_epochs:
            self._run_agent_train_epoch()

        if self._epoch >= self._n_precollect_epochs:
            # We start training model after precollecting data with the agent
            # as well (i. e. we do not start training with only one batch of
            # episodes, and rather we wait n_precollect_epochs).
            if self._fine_tune_on_agent_data:
                self._run_model_train_epoch('model_fine_tune', self._epoch)

        if self._epoch == self._n_precollect_epochs:
            # Save gin operative config into a file. "Operative" means the part
            # that is actually used in the experiment. We need to run an full
            # epoch (data collection + training) first, so gin can figure that
            # out.
            self._save_gin()

        self._epoch += 1

    def _run_agent_train_epoch(self):
        time_stamp_before_train = time.time()
        metrics = self._trainer.train_epoch(self._network)
        metric_logging.log_scalar_metrics(
            'run_time',
            self._epoch,
            {'agent_train': time.time() - time_stamp_before_train}
        )
        metric_logging.log_scalar_metrics('agent_train',
                                          self._epoch,
                                          metrics
                                          )

    def _run_episode_batch(self):
        time_stamp_before_run_agent = time.time()
        if self._model_network is not None:
            model_params = self._model_network.params
        else:
            model_params = None
        episodes = self._batch_stepper.run_episode_batch(
            self._network.params,
            model_params,
            epoch=max(0, self._epoch),
            time_limit=self._episode_time_limit
        )
        metric_logging.log_scalar_metrics(
            'run_time',
            self._epoch,
            {'episode_batch': time.time() - time_stamp_before_run_agent}
        )
        return episodes

    def _log_episodes_metrics_and_images(self, episodes):
        metric_logging.log_scalar_metrics(
            'agent',
            self._epoch,
            self._agent_class.compute_metrics(episodes)
        )
        metric_logging.log_scalar_metrics(
            'env',
            self._epoch,
            self._logging_env.compute_metrics(episodes)
        )
        metric_logging.log_scalar_metrics(
            'episode',
            self._epoch,
            self._compute_episode_metrics(episodes)
        )
        metric_logging.log_scalar_metrics(
            'episode_rolling',
            self._epoch,
            self._compute_metrics_across_epochs(episodes)
        )

        self._maybe_log_n_experienced_states(episodes)
        self._maybe_update_and_log_graph_distance_metrics(episodes)
        self._maybe_log_trainable_env_model_metrics_and_images(episodes)
        self._maybe_log_single_episode_visualization(episodes)
        self._maybe_log_agent_visit_heat_map(episodes)
        self._maybe_log_transitions_visualizations(episodes)

    def _maybe_log_n_experienced_states(self, episodes):
        if self._log_n_experienced_states:
            self._experienced_states_logger.update_and_log(
                episodes, self._epoch)

    def _maybe_update_and_log_graph_distance_metrics(self, episodes):
        if self._log_graph_distances:
            self._graph_logger.update_and_log(episodes, self._epoch)

    def _maybe_log_trainable_env_model_metrics_and_images(self, episodes):
        if self._model_network is not None:
            metrics = self._compute_trainable_model_metrics(episodes)
            metric_logging.log_scalar_metrics(
                'episode_model',
                self._epoch,
                metrics
            )

            if self._log_false_positive_images:
                time_stamp_before_log = time.time()
                self._log_transitions_with_false_positive_rewards(episodes)
                time_diff = time.time() - time_stamp_before_log
                metric_logging.log_scalar_metrics(
                    'run_time',
                    self._epoch,
                    {'false_positive_images': time_diff}
                )

    def _maybe_log_transitions_visualizations(self, episodes):
        if (
            self._log_other_images_every_n_epochs is not None and
            self._epoch % self._log_other_images_every_n_epochs == 0
        ):
            time_stamp_before_log = time.time()
            self._log_hard_transitions_predictions_visualizations(episodes)
            self._log_top_priority_transitions()
            time_diff = time.time() - time_stamp_before_log
            metric_logging.log_scalar_metrics(
                'run_time',
                self._epoch,
                {'other_images': time_diff}
            )

    def _maybe_log_agent_visit_heat_map(self, episodes):
        if (
            self._log_heat_map_every_n_epochs is not None and
            self._epoch % self._log_heat_map_every_n_epochs == 0
        ):
            time_stamp_before_log = time.time()
            self._log_agent_visit_heat_map(
                episodes, self._log_detailed_heat_map
            )
            time_diff = time.time() - time_stamp_before_log
            metric_logging.log_scalar_metrics(
                'run_time',
                self._epoch,
                {'heat_map': time_diff}
            )

    def _maybe_log_single_episode_visualization(self, episodes):
        if (
            self._log_rollout_every_n_epochs is not None and
            self._epoch % self._log_rollout_every_n_epochs == 0
        ):
            time_stamp_before_log = time.time()
            self._log_single_episode_visualization(episodes)
            time_diff = time.time() - time_stamp_before_log
            metric_logging.log_scalar_metrics(
                'run_time',
                self._epoch,
                {'single_rollout': time_diff}
            )

    def _run_model_train_epoch(self, train_phase, train_epoch):
        time_stamp_before_train = time.time()
        model_train_metrics = self._model_trainer.train_epoch(
            self._model_network
        )
        time_diff = time.time() - time_stamp_before_train
        metric_logging.log_scalar_metrics(
            'run_time',
            train_epoch,
            {train_phase: time_diff}
        )
        metric_logging.log_scalar_metrics(
            train_phase,
            train_epoch,
            model_train_metrics
        )

    def _log_single_episode_visualization(self, episodes):
        transition_batch = episodes[0].transition_batch
        rollout_visualization = self._logging_env.visualize_transitions(
            transition_batch
        )
        for i, next_observation_preds in enumerate(rollout_visualization):
            metric_logging.log_image(
                f'episode_model/rollout_epoch_{self._epoch}',
                i, next_observation_preds
            )

    def _log_hard_transitions_predictions_visualizations(self, episodes):
        trainable_env_info = episodes[0].trainable_env_info
        if trainable_env_info is not None:
            hard_transitions = self._logging_env.visualize_hard_transitions(
                trainable_env_info
            )
            for name_t, transition_viz in hard_transitions.items():
                metric_logging.log_image(
                    f'episode_model/transition_{name_t}',
                    self._epoch, transition_viz
                )

    def _log_agent_visit_heat_map(self, episodes, log_detailed_heat_map=False):

        self._logging_env.log_visit_heat_map(
            self._epoch, episodes,
            log_detailed_heat_map,
            metric_logging
        )

    def _log_agent_visit_heat_map_toy_mr(self, episodes,
                                         log_detailed_heat_map=True):
        state_visit_freq = {}
        invalid_transitions = {}
        for episode in episodes:
            for observation in episode.transition_batch.observation:
                state = self._logging_env.obs2state(observation)
                if state not in state_visit_freq:
                    state_visit_freq[state] = 0
                state_visit_freq[state] += 1

            for obs, action, next_obs, next_obs_preds in zip(
                episode.transition_batch.observation,
                episode.transition_batch.action,
                episode.transition_batch.next_observation,
                episode.transition_batch.agent_info['children_observations'],
            ):
                next_obs_pred = next_obs_preds[action]
                if not (next_obs == next_obs_pred).all():
                    state = self._logging_env.obs2state(obs)
                    next_state_pred = self._logging_env.obs2state(next_obs_pred)
                    if state not in invalid_transitions:
                        invalid_transitions[state] = {}
                    invalid_transitions[state][action] = next_state_pred

        visit_heat_map = self._logging_env.render_visit_heat_map(
            state_visit_freq, invalid_transitions, separate_by_keys=False
        )
        metric_logging.log_image(
            f'episode_model/visit_heat_map',
            self._epoch, visit_heat_map
        )
        if log_detailed_heat_map:
            visit_heat_map = self._logging_env.render_visit_heat_map(
                state_visit_freq, invalid_transitions, separate_by_keys=True
            )
            metric_logging.log_image(
                f'episode_model/visit_heat_map_detailed',
                self._epoch, visit_heat_map
            )

        novel_transitions = {}
        for episode in episodes:
            if 'novelty_based_action' in episode.transition_batch.agent_info:
                for obs, action, next_obs, is_novel in zip(
                    episode.transition_batch.observation,
                    episode.transition_batch.action,
                    episode.transition_batch.next_observation,
                    episode.transition_batch.agent_info['novelty_based_action'],
                ):
                    if is_novel:
                        state = self._logging_env.obs2state(obs)
                        next_state = self._logging_env.obs2state(next_obs)
                        if state not in novel_transitions:
                            novel_transitions[state] = {}
                        novel_transitions[state][action] = next_state

        visit_heat_map = self._logging_env.render_visit_heat_map(
            state_visit_freq, novel_transitions, separate_by_keys=False
        )
        metric_logging.log_image(
            f'episode_model/visit_heat_map_novelty_actions',
            self._epoch, visit_heat_map
        )
        if log_detailed_heat_map:
            visit_heat_map = self._logging_env.render_visit_heat_map(
                state_visit_freq, novel_transitions, separate_by_keys=True
            )
            metric_logging.log_image(
                f'episode_model/visit_heat_map_novelty_actions_detailed',
                self._epoch, visit_heat_map
            )

    def _log_top_priority_transitions(self):
        if (self._log_n_top_transitions is not None and
                self._model_trainer.replay_sample_mode == 'priority'):
            samples = self._model_trainer.top_priority_samples(
                self._log_n_top_transitions
            )
            for bucket_name, (batch, priorities) in samples.items():
                inputs, targets = zip(*batch)
                inputs_batch = np.stack(inputs)
                preds = self._model_network.predict(inputs_batch)
                preds_unstacked = nested_unstack(preds)
                batch_with_preds = zip(inputs, targets, preds_unstacked)
                visualizations = self._logging_env.visualize_replay_buffer(
                    batch_with_preds, priorities
                )
                for i, transition_viz in enumerate(visualizations):
                    metric_logging.log_image(
                        f'model/top_priority_{bucket_name}_epoch_'
                        f'{self._epoch}',
                        i, transition_viz
                    )

    def run(self):
        """Runs the main loop."""
        if self._n_epochs is None:
            epochs = itertools.repeat(None)  # Infinite stream of Nones.
        else:
            epochs = range(self._n_epochs)

        if self._n_model_precollect_epochs > 0:
            self._generate_random_episodes()

        if self._n_model_pretrain_epochs is not None:
            for pretrain_epoch in range(self._n_model_pretrain_epochs):
                self._run_model_train_epoch('model_pretrain', pretrain_epoch)

            if self._clear_model_replay_after_pretrain:
                self._model_trainer.clear_experience()

        for _ in epochs:
            self.run_epoch()


def _parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--output_dir', required=True,
        help='Output directory.')
    parser.add_argument(
        '--config_file', action='append',
        help='Gin config files.'
    )
    parser.add_argument(
        '--config', action='append',
        help='Gin config overrides.'
    )
    parser.add_argument(
        '--mrunner', action='store_true',
        help='Add mrunner spec to gin-config overrides and Neptune to loggers.'
        '\nNOTE: It assumes that the last config override (--config argument) '
        'is a path to a pickled experiment config created by the mrunner CLI or'
        'a mrunner specification file.'
    )
    parser.add_argument(
        '--tensorboard', action='store_true',
        help='Enable TensorBoard logging: logdir=<output_dir>/tb_%m-%dT%H%M%S.'
    )
    return parser.parse_args()


if __name__ == '__main__':
    args = _parse_args()

    gin_bindings = args.config

    if args.mrunner:
        from alpacka.utils import mrunner_client  # Lazy import
        spec_path = gin_bindings.pop()

        specification, overrides = mrunner_client.get_configuration(spec_path)
        gin_bindings.extend(overrides)

        try:
            neptune_logger = mrunner_client.configure_neptune(specification)
            metric_logging.register_logger(neptune_logger)

        except NeptuneAPITokenException:
            print('HINT: To run with Neptune logging please set your '
                  'NEPTUNE_API_TOKEN environment variable')

    if args.tensorboard:
        from alpacka.utils import tensorboard  # Lazy import

        tensorboard_logger = tensorboard.TensorBoardLogger(args.output_dir)
        metric_logging.register_logger(tensorboard_logger)

    gin.parse_config_files_and_bindings(args.config_file, gin_bindings)
    runner = Runner(args.output_dir)
    runner.run()
